<!DOCTYPE html>
<html lang="en" >
<link rel="stylesheet" href="https://kipp.ly/themes/purple.css">
<head>
  <meta charset="utf-8" />
  <meta name="referrer" content="no-referrer">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

  <title>Transformer Taxonomy (the last lit review) | kipply&#x27;s blog</title>
<meta property="og:title" content="Transformer Taxonomy (the last lit review) | kipply&#x27;s blog" />
<meta name="twitter:title" content="Transformer Taxonomy (the last lit review) | kipply&#x27;s blog" />

  <meta name="description" content="kipply&#x27;s blog about stuff she does or reads about or observes">
  <meta property="og:description" content="kipply&#x27;s blog about stuff she does or reads about or observes">
  <meta name="twitter:description" content="kipply&#x27;s blog about stuff she does or reads about or observes">

  <meta property="og:site_name" content="kipply&#x27;s blog" />
  <meta property="og:url" content="https:&#x2F;&#x2F;kipp.ly" />

  <link
      rel="preload"
      href="https://fonts.cdnfonts.com/css/linux-libertine-o"
      as="style"
      onload="this.onload=null;this.rel='stylesheet'"
  />
  <link
      rel="preload"
      href="https://fonts.googleapis.com/css2?family=Montserrat:wght@500&display=swap"
      as="style"
      onload="this.onload=null;this.rel='stylesheet'"
  />
  <link
      rel="preload"
      href="https://fonts.googleapis.com/css2?family=Dancing+Script&family=Rubik&display=swap"
      as="style"
      onload="this.onload=null;this.rel='stylesheet'"
  />
  <link
      rel="preload"
      href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400;600&display=swap"
      as="style"
      onload="this.onload=null;this.rel='stylesheet'"
  />
  <noscript>
      <link
          href="https://fonts.cdnfonts.com/css/linux-libertine-o"
          rel="stylesheet"
          type="text/css"
      />
      <link
          href="https://fonts.googleapis.com/css2?family=Montserrat:wght@500&display=swap"
          rel="stylesheet"
          type="text/css"
      />
      <link
          href="https://fonts.googleapis.com/css2?family=Dancing+Script&family=Rubik&display=swap"
          rel="stylesheet"
          type="text/css"
      />
      <link
          href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400;600&display=swap"
          rel="stylesheet"
          type="text/css"
      />
  </noscript>

  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter&display=swap" rel="stylesheet">

  <script type="text/javascript">
  let fetchStyle = function() {
        const themes = ['blue', 'deeppurple', 'green', 'lightblue', 'pink', 'purple', 'red'];
    const theme = "https://kipp.ly/themes/" + themes[Math.floor(Math.random() * themes.length)] + ".css";

    let link = document.createElement('link');
    link.type = 'text/css';
    link.rel = 'stylesheet';
    link.onload = function() {
      document.documentElement.setAttribute("style", "display:auto");
     };
    link.href = theme;

    let headScript = document.querySelector('script');
    headScript.parentNode.insertBefore(link, headScript);
  }
  fetchStyle()
  </script>

  <link rel='icon' type='image/x-icon' href="https://kipp.ly/favicon.ico" />

  <link rel="alternate" type="application/atom+xml" title="kipply&#x27;s blog" href="https://kipp.ly/atom.xml">

  

</head>
<body>
  <header class="header">
    <h1 class="title-link"><a href="https:&#x2F;&#x2F;kipp.ly">kipply&#x27;s blog</a></h1>
    <div class="header-links">
      <a href="https://twitter.com/kipperrii">twitter</a>,
      your_name at kipp dot ly
    </div>
  </header>

  <main id="main" class="main">
    


    

<header class="source-serif">
  <h1>Transformer Taxonomy (the last lit review)</h1>
  <div>
    <div class="article-meta">
      <time datetime="2023-03-30">
        2023-03-30
      </time>
      
      (21 min read)
    </div>
  </div>
</header>
<article class="article source-serif">
  <div class="page-body">
    <!--  -->
    <section id="js-article" class="article-body">
      
<p>This document is my running literature review for people trying to catch up on AI. It covers 22 models, 11 architectural changes, 7 post-pre-training techniques and 3 training techniques (and 5 things that are none of the above). Everything is very loosely in order of importance and somewhat uniqueness. All papers will link to the actual PDF and not the ArXiv page and the selection is mostly curated based on things I know about. Systems/performance and alignment are excluded for this one because they’re my favourite and I’d want to do it more justice. Alignment research is really important, I hope to do it justice some day! Also probably not all the papers in the model list are worth reading.</p>
<h2 id="1-models">1. Models</h2>
<p>If a property is unspecified it’s either undisclosed or follows approximately the standard GPT recipe.</p>
<p><strong><strong><strong><strong><strong>GPT-3</strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2005.14165.pdf">paper</a>] — <strong><strong><strong><strong><strong><strong>175B params, 96 layers, 12288 embd dim, 96 heads — OpenAI May 2020</strong></strong></strong></strong></strong></strong></p>
<p>This was a seminal paper for large language models, following the <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2 paper</a> (2018) and the <a href="https://arxiv.org/pdf/2001.08361.pdf">scaling laws paper</a>. It was trained on a 300B token dataset consisting mostly of filtered Common Crawl, along with some books, webtext and Wikipedia. BPE tokenizer (same from GPT-2). 2048 context length. Alternates dense and sparse attention layers. Warms up to 0.6 × 10^−4 learning rate in the first 375M toks, cosine decayed to 10% after 260B toks. Batch size ramp from 32k toks to 3.2M toks over the first 12B tokens. 4x MLP projection ratio as done in the <a href="https://arxiv.org/pdf/1706.03762.pdf">2017 transformer paper</a>. 50k vocab size. Many of these characteristics (e.g. embd dim = 128 * layers, 4x MLP projection ratio, and LR and batch size ramp) form a standard recipe that has been reused by later models.</p>
<blockquote>
<p>there’s a probably-typo in Table 2.1 that documents the hyperparameters, where GPT-3 13B is labelled as having an embedding dimension of 5140 which should probably be 5120</p>
</blockquote>
<p><strong><strong><strong><strong><strong>GPT-4</strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2303.08774.pdf">technical report</a>] — <strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Released March 2023, finished pre-training August 2022</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>Man, feels awkward to write a pathetic summary of something this big, but here goes: GPT-4 is a model available through OpenAI of unknown architecture (other than that it’s GPT-like, though they only technically specify transformer-like). The technical report contains mostly evals (which performed well of course), as well as the results of their continued scaling which are accurately extrapolated from smaller models. The report also documents safety mitigation and has a demo of their multi-modal capabilities of GPT-4 which seem trained in à la Flamingo. It also has the best Acknowledgements section of all time.</p>
<p><strong><strong><strong><strong><strong><strong><strong>Gopher</strong></strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2112.11446.pdf">paper</a>] — <em>280B params, 260B non-embedding params, 80 layers, 16384 embd dim, 128 heads — DeepMind Dec 2021</em></p>
<p>DeepMind’s first large language model release in 2021. It uses an RMSNorm instead of a LayerNorm, uses a relative positional encoding scheme from Transformer-XL instead of an absolute positional encoding, which is why there are so many embedding parameters. Tokenizes with SentencePiece, vocab size 32k. Trained on 300B tokens, with half being from MassiveText which was collected for Gopher, along with books, Common Crawl, Wikipedia, news and Github. Note that Gopher was actually trained end of 2020 and released a year later.</p>
<p><strong><strong><strong><strong><strong><strong>AlphaCode</strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2203.07814.pdf">paper</a>] <em>— 41B,  8 encoder layers, 56 decoder layers, 6144 embd dim — DeepMind Feb 2022</em></p>
<p>A model trained on 715GB(967B tokens) of code to do competitive programming. The only model on this list with an encoder-decoder architecture, it treated contest programming as a translation task (problem statement → solution) to gain bidirectionality. It uses 1536 tokens in the encoder and 768 tokens in the decoder. Uses multi-query attention, and generates thousands of samples at inference time and then selects a subset of solutions to submit.</p>
<p><strong><strong><strong><strong><strong>RETRO</strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2112.04426.pdf">paper</a>] — <em>7B parameters — DeepMind Feb 2022</em></p>
<p>Retrieval is the general technique if giving a model a database to look up while doing inference. RETRO was the inaugural retrieval paper for transformers, using a 2T token database. It embeds the token-database in chunks using a pretrained BERT-style model and then performs chunked cross-attention to nearest neighbors in the database during training and inference</p>
<p><strong><strong><strong>GPT-3.5</strong></strong></strong></p>
<p>[<a href="https://beta.openai.com/docs/model-index-for-researchers">docs</a>] — <em><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>architecture unknown — OpenAI Mar 2022</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></em>

OpenAI delineates three models as GPT-3.5, specifically anything in the <code>davinci-002</code> or <code>davinci-003</code> family. <code>code-davinci-002</code> is the base model, <code>text-davinci-002</code> is a version with FeedME non-RL instruction tuning, and <code>text-davinci-003</code> is an InstructGPT with RLHF. There is an InstructGPT paper that trains an RLHF model and does not mention FeedME, and though <code>text-davinci-002</code> is an InstructGPT model it does not use RLHF. The <code>davinci</code> model on the OpenAI API is noted to be the 175B model in the 2020 paper, but it’s never confirmed whether <code>davinci-002</code> is the same size.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Chinchilla</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2203.15556.pdf">paper</a>] — <em>70B params, 80 layers, 8192 embd dim, 64 heads — DeepMind Mar 2022</em></p>
<p>With the paper titled &quot;Training Compute-Optimal Large Language Models”, new and improved scaling laws were introduced. Chinchilla is trained with 1.5T tokens (similar dataset as Gopher) and same amount of compute as Gopher, yet outperforms it. Results in scaling laws that have parameters and tokens linearly increase at a 20:1 token to parameter ratio. Learning rate adjusts with a cosine schedule. <a href="https://arxiv.org/pdf/2201.11990.pdf">Megatron Turing NLG</a> and <a href="https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf">Jurassic J-1 Jumbo</a> are two other large models that aren’t documented here as they are not Chinchilla optimal and aren’t uniquely significant.</p>
<p><strong><strong>Flamingo</strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2204.14198.pdf">paper</a>] <em>— 80B params — DeepMind Apr 2022</em></p>
<p>Flamingo is a multi-modal (text and image) model. It only generates text, and image inputs are run through a vision encoder (435M params), and cross-attention is used to attend to those outputs. It also uses a resampler (194M params) after the vision encoder to produce a fixed (small) number of visual tokens no matter the number of input features. They build on frozen Chinchilla models, the 80B params come from the cross-attention layers added to the 70B Chinchilla model. <a href="https://arxiv.org/pdf/2209.06794.pdf">PaLI</a> is a Google model that follows up on image/language multimodal.</p>
<p><strong><strong><strong><strong>Gato</strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2205.06175.pdf">paper</a>] <em>— 1.18B params — May 2022</em></p>
<p>Gato is a generalist agent, sort of a follow up to Flamingo with more modalities. It uses images and text, as well as button-press data formatted into tokens, as well as encodings of continuous data from robotics propioception, trying to use as little data as possible for additional tasks. The tasks include robotics stacking tests, image captioning, and Atari.</p>
<p><strong>Anthropic LM</strong></p>
<p>[<a href="https://arxiv.org/pdf/2112.00861.pdf">paper</a>] <em>— 52B params, 64 layers, 8192 embd dim — Anthropic Dec 2021</em></p>
<p>Trained on 400B tokens, though in a <a href="https://arxiv.org/pdf/2207.05221.pdf">later, post-Chinchilla paper</a>, Anthropic used a model with the same architecture trained for 850B tokens. And in yet another later paper on <a href="https://arxiv.org/pdf/2302.07459.pdf">moral self-correction</a>, a 175B with no other specified properties is used.</p>
<p><strong><strong>PaLM</strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2204.02311.pdf">paper</a>] <em>— 540B params, 118 layers, 18432 embd dim, 48 heads — Google Apr 2022</em></p>
<p>Current (as of Jan 2023) largest publicly known dense language model, unfortunately pre-Chinchilla. PaLM activates with SwiGLU, uses parallel attention, multi-query attention, rotary embeddings and uses the same matrices for input and output embeddings. No biases were used and a SentencePiece tokenizer with 256k tokens was used. PaLM was trained on 780B tokens, on a similar dataset as LaMDA and GLaM.</p>
<p><strong><strong><strong><strong><strong><strong>GPT-NeoX</strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://github.com/EleutherAI/gpt-neox">github</a>][<a href="https://arxiv.org/pdf/2204.06745.pdf">paper</a>] — <strong><strong><strong><strong><strong>20B params — Eleuther AI Feb 2022</strong></strong></strong></strong></strong></p>
<p>An Eleuther open-sourced model, trained on GPUs with <a href="https://www.deepspeed.ai/">DeepSpeed</a> (microsoft) and <a href="https://github.com/NVIDIA/Megatron-LM">Nvidia Megatron</a>. It uses the same architectural modifications that GPT-J had and is trained on the entirety of Pile, 400B tokens.</p>
<p><strong><strong><strong><strong><strong><strong>GPT-J</strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b">github</a>] — <em>6.7B params — Eleuther AI Jul 2021</em></p>
<p>Notable for being a fully open-sourced model, while matching the 6.7B performance from the GPT-3 paper. Trained on TPUs, and done with rotary embeddings, parallel attention. Only dense attention layers are used to reduce complexity. It was trained on <a href="https://pile.eleuther.ai/">the Pile</a>, an open dataset created by Eleuther AI which contains 22 smaller datasets including Common Crawl, OpenWebText, books and papers.</p>
<p><strong><strong><strong><strong>GLaM</strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2112.06905.pdf">paper</a>] <em>— 1.2T parameters — Google Dec 2021</em></p>
<p>Named “Generalist Language Model”, GLaM is a Mixture-of-Experts (MoE) model, where parameters are sparsely activated. It has 64 experts per layer, with each token activating 96.6B parameters. Each layer has a gating unit which selects one two of the 64 MLPs per each token</p>
<p><strong><strong><strong><strong><strong><strong>LaMDA</strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2201.08239.pdf">paper</a>] — <em>137B params, 64 layers, 8192 embd dim, 128 heads  — Google (demoed at I/O May 2021; paper posted Jan 2022)</em></p>
<p>Dialog model made to follow <a href="https://arxiv.org/pdf/2001.09977.pdf">Meena</a>. A 2.81T dataset with a lot of dialog/forums (encoded with a 32k vocab size SentencePiece tokenizer) is specified. The base model is sometimes called LaMDA GLM or GLM-137B; LaMDA itself adds a lot of dialog finetuning on top.</p>
<blockquote>
<p>Though it’s explicit how many tokens the model was trained for. It does specify 1024 TPUv3 chips at 56.5% utilisation for 57.7 days, batch size 256k, probably bf16, and arithmetic says that would be about 900B of the 2.81T tokens.</p>
</blockquote>
<p><strong><strong><strong><strong><strong><strong>Switch</strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2101.03961.pdf">paper</a>] <em>— 1T parameters — Google Jun 2022</em></p>
<p>An improvement on GLaM, SwitchTransformer only routes to one expert, reducing the amount of compute. It using a different routing mechanism, with the main update being that routing to a single expert works.</p>
<p><strong><strong><strong><strong><strong>BLOOM</strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2211.05100.pdf">paper</a>] — <em>176B params, 70 layers, 14336 embd dim, 112 heads — HuggingFace July 2022</em></p>
<p>Current largest open-source model. Trained on a HuggingFace corpus called ROOTS, which is 498 HuggingFace datasets. The model was trained for 366B tokens. Positional encodings was done with ALiBi. 250k vocab size BPE tokenizer, to help accommodate for multilingual data.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong>Galactica</strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2211.09085.pdf">paper</a>] <em>— 120B parameters — Meta Nov 2022</em></p>
<p>Galactica is a science model pretrained mostly on papers, along with small amounts of code, other knowledge-based data and a bit of common crawl. It uses a <code>&lt;work&gt;</code> token to encode working memory, as well as special tokens for citations.</p>
<p><strong><strong><strong><strong><strong>LLaMa</strong></strong></strong></strong></strong></p>
<p>[<a href="https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=ov6yTHfLfNQAX82vXIA&amp;_nc_ht=scontent-sjc3-1.xx&amp;oh=00_AfAg4KoJmp5lBEyThQ9XAh24xKRPZ-wVH1UWh4euhxSy8w&amp;oe=63FFCFA2">paper</a>] <em>— 65B parameters — Meta Feb 2023</em></p>
<p>Chinchilla replication. Fairly standard training mix of mostly Common Crawl.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong>Jurassic J1-Grande v2</strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2204.14198.pdf">paper</a> for v1][<a href="https://crfm.stanford.edu/helm/latest">helm evals</a>] <em>— 17B parameters — AI21 Dec 2022</em></p>
<p>No information other than the Helm results, which look really good for the size!</p>
<p><strong><strong><strong>OPT</strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/2205.01068.pdf">paper</a>][<a href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf">train logbook</a>] — <em>175B params, same arch as GPT-3 — Meta May 2022</em></p>
<p>Meta replication of GPT-3. Trains on the Pile and PushShift reddit, for only 180B tokens.</p>
<blockquote>
<p>The Meta papers aren’t at all connected projects. LLama, OPT and Galactica share only one author of 41.</p>
</blockquote>
<p><strong>GLM-130B</strong></p>
<p>[<a href="https://arxiv.org/pdf/2210.02414.pdf">paper</a>] — <em>130B params — Tsinghua University Oct 2022</em></p>
<p>GLM is an open-sourced bilingual (Chinese and English) model. It uses rotary embeddings, DeepNorm, and activates the MLP with GeGLU. It notably inferenced in INT4 (where other models like BLOOM and OPT had quantized to INT8). It also includes prompts in pretraining Instead of the standard GPT architecture, it uses GLM for bidirectional attention.</p>
<h2 id="2-architectural-changes">2. Architectural Changes</h2>
<p><strong>Multi-Query Attention</strong></p>
<p>This <a href="https://arxiv.org/pdf/1911.02150.pdf">Noam Shazeer solo paper</a>, where the key and values are shared across heads, greatly reducing the amount of memory required at inference time, improving latency and throughput. It’s a perfectly concise barely 9 page paper complete with code and results so it feels silly to describe further. AlphaCode and PaLM both use multi-query.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Sparse Attention</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>[<a href="https://arxiv.org/pdf/1904.10509.pdf">sparse transformer paper</a>] — Sparse attention is a mechanism where attention is not applied to all previous tokens. It describes two styles of the SparseTransformer, strided where it looks at the last N tokens, and then fixed where sections of tokens in the sequence are attended to. In the GPT-3 paper, the model is described to have alternating dense and “locally banded” sparse layers.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Mixture-of-Experts</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>There’s a lot more lore on MoE, and I already gave the one-liner in describing GLaM and Switch so here I’ll just give an good initial literature list!</p>
<ul>
<li>the <a href="https://arxiv.org/abs/1701.06538">original MoE paper</a> from 2017 on LSTMs</li>
<li>Deepmind Scaling Laws <a href="https://arxiv.org/pdf/2202.01169.pdf">paper</a> for MoE</li>
<li>Meta <a href="https://arxiv.org/pdf/2112.10684.pdf">paper</a> that trains a 1.1T param MoE</li>
<li>A <a href="https://arxiv.org/pdf/2202.08906.pdf">large</a> <a href="https://arxiv.org/pdf/2202.09368.pdf">pool</a> <a href="https://arxiv.org/pdf/2205.10937.pdf">of</a> <a href="https://arxiv.org/pdf/2202.08906.pdf">Google</a> <a href="https://openreview.net/pdf?id=23ZjUGpjcc">papers</a></li>
</ul>
<p><strong><strong><strong><strong><strong><strong><strong>FlashAttention</strong></strong></strong></strong></strong></strong></strong></p>
<p><a href="https://arxiv.org/pdf/2205.14135.pdf">FlashAttention</a> is an architectural change to do attention with less memory access (most of costs in most cases). It tiles and incrementally performs the softmax reduction and avoids storing the whole intermediate attention matrix for the backwards pass. The paper cites 1.7x training speedup compared to megatron and up to over 4x on inference (with the multiplier increasing with longer context lengths). The same sort of approach achieving O(log_n) memory was done earlier on TPUs in <a href="https://arxiv.org/pdf/2112.05682.pdf">this paper</a>.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Encoder+Decoder</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>

A la original <a href="https://arxiv.org/pdf/1706.03762.pdf">transformer paper</a>, the encoder decoder architecture was originally made for translation tasks. Where the classic GPT architecture are alternating attention and mlp blocks, the original transformer had an encoder block which was attention → mlp and a decoder block which was masked attention → encoder-decoder attention → mlp. This is still a reasonable architecture to many kinds of sequence-to-sequence tasks, such as AlphaCode or <a href="https://arxiv.org/pdf/1910.10683.pdf">T5</a> (Google, 2019, 11B params).</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Parallel Attention</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p><a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM</a> uses parallel attention (poorly named) where the model is trained with the attention and MLP layers run in parallel, taking the same vectors. This makes it so that you can do your attention and feed-forward matmuls together to increase arithmetic intensity for better performance (15% on PaLM). GPT-J also uses it.</p>
<p><strong>Activation Alternatives: GeGLU, SwiGLU, SoLU</strong></p>
<p>The <a href="https://arxiv.org/pdf/1706.03762.pdf">original transformer paper</a> uses ReLU (Rectified Linear Unit) to activate the MLP block. It does the simple x if &gt; x = 0 else 0 in between the two linear transformations (matmuls). Intuitively, this is a bit too no-brained. GeLU (Gaussian error) is similar to ReLU but smooths it out a bit. SoLU (Softmax) introduced by <a href="https://transformer-circuits.pub/2022/solu/index.html">this Anthropic paper</a>, is simply <code>x*softmax(x)</code> and is used to improve the interpretability of models. SwiGLU is the most sophisticated of the listed, and is a <a href="https://arxiv.org/pdf/2002.05202.pdf">Noam Shazeer solo paper</a>, as it came through “divine benevolence”. It builds upon gated linear units (meant to be more stable than ReLU) and does the swish operation before the GLU. Like GeLU, it softens out the ReLU and allows some values to be under zero.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>LayerNorm Alternatives: DeepNorm, RMSNorm</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>LLMs norm twice per block (once for attention, once to feed-forward), which does some normalisation function to improve training. <a href="https://arxiv.org/pdf/2203.00555.pdf">DeepNorm</a> and <a href="https://arxiv.org/pdf/1910.07467.pdf">RMSNorm</a> are alternatives. RMSNorm (Root Mean Square) is simply the square root of the mean of the values. There’s also a batch norm that’s inefficient and seems silly to use.</p>
<p><strong>RoPE</strong></p>
<p>[<a href="https://arxiv.org/pdf/2104.09864.pdf">paper</a>][<a href="https://blog.eleuther.ai/rotary-embeddings/">blog post</a>] — I don’t want to try to summarize this one because there’s a good tl;dr in the blog post.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>BPE vs SentencePiece Tokenizers</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>

[[bpe](https://huggingface.co/course/chapter6/5?fw=pt)][<a href="https://github.com/google/sentencepiece">sentence piece</a>] — Byte Pair Encodings are the default for most language models, being used by the original GPT paper, GPT-3 and presumably (based on the API) GPT-3.5. An obvious reason to not use plain BPE (and instead use SentencePiece) is if your distribution doesn’t contain space separated words, as AlphaCode, GLM (Chinese) and PaLM (explicitly because multilingual) did.</p>
<p><strong><strong><strong><strong><strong>ALiBi</strong></strong></strong></strong></strong></p>
<p><a href="https://arxiv.org/pdf/2108.12409.pdf">Attention with Linear Biases</a> is a long context positional embedding scheme to support extrapolation to longer lengths, by biasing (linearly) the qk scores according to their distance. BLOOM uses ALiBi and Galactica tried it though didn’t go through with it.</p>
<h2 id="3-post-pre-training-techniques">3. Post-Pre-Training Techniques</h2>
<p><strong><strong>RLHF with PPO</strong></strong></p>
<p>In RLHF, a reward model is trained, where the labeler evaluates an array of model generations. Then the PPO (<a href="https://arxiv.org/pdf/1707.06347.pdf">proximal policy optimization</a>) is used for the RL, where the policy generates an output evaluated by the reward model to improve on the policy.</p>
<p>Deepmind’s <a href="https://arxiv.org/pdf/2209.14375.pdf">Sparrow</a>, as well as Anthropic’s LMs are trained with RL(AI|H)F are have dialog interfaces. <a href="https://arxiv.org/pdf/2112.09332.pdf">WebGPT</a> was was trained with RLHF, as was <a href="https://storage.googleapis.com/deepmind-media/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes/Teaching%20language%20models%20to%20support%20answers%20with%20verified%20quotes.pdf">GopherCite</a> (which called RLHPreferences). I’d say the origination was <a href="https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html">Christiano 2017</a>, preceding any LLM stuff, followed by 2020 <a href="https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf">summarizing from human feedback</a>, along with the PPO paper.</p>
<p><strong>Constitutional</strong></p>
<p>An extension of RLHF, <a href="https://arxiv.org/pdf/2212.08073.pdf">Constitutional</a> is basically RLAIF, though actually called “CAI”. It has a supervised learning phase where a helpful-only AI is used to generate adversarial prompts. The assistant then iterates on its own response based on the provided constitution (a short set of values for the model to follow in the form of a string). Then finetuning is done on those responses. The second stage then is like RLHF with PPO, except substituting the AI feedback.</p>
<p><strong><strong><strong><strong><strong><strong><strong>Minerva</strong></strong></strong></strong></strong></strong></strong></p>
<p>Released in 2022 June from the Blueshift team, <a href="https://arxiv.org/pdf/2206.14858.pdf">Minerva</a> is a finetuned model on math and science data, particularly well-executed. It’s a 62/540B finetuned model from PaLM, with datasets from ArXiV and some websites that were carefully preprocessed to preserve mathematical formatting.</p>
<p><strong><strong><strong><strong><strong>Codex</strong></strong></strong></strong></strong></p>
<p>Launched in July 2021 (and resulted in Github Copilot), <a href="https://arxiv.org/pdf/2107.03374.pdf">Codex</a> is a finetune on 100B tokens of code (in this case, publicly available Github code). The paper also debuted HumanEval, human written code evals. This paper most notably demonstrates that code data is really important for code performance, as GPT-J was outperforming 3 at code. They also added some tokens for code, which improved the compression by 30%.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Just Finetune on CoTed Outputs</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>I forgot which paper did this but its like they finetuned their model on chain of thought outputs from the model, and it did better. Expected, but notable result.</p>
<p><strong><strong>FeedME (SFT)</strong></strong></p>
<p>Described in <a href="https://arxiv.org/pdf/2203.02155.pdf">Instruct GPT paper</a> (though it is not necessarily the origination, which is closer <a href="https://arxiv.org/abs/1909.08593">to this</a>). Supervised Fine-Tuning uses human-generated content which is then used to fine-tune the pre-trained model. The paper finds that SFT performs better than base pre-trained models but RLHF performs better than SFT.</p>
<p><strong>FLAN</strong></p>
<p><a href="https://arxiv.org/pdf/2109.01652.pdf">Flan</a> is an instruction-tuned model (finetuned on instruction-formatted nlp tasks) that results in improved zero-shot performance.</p>
<h2 id="4-training-techniques">4. Training Techniques</h2>
<p><strong>Being</strong> <strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Good at Setting Hyperparameters</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>

There is obviously no one paper for this, but obviously getting the hyperparameters right is pretty important. Some baseline is available by reading papers, with the most notable probably being <a href="https://arxiv.org/pdf/2203.15556.pdf">Chinchilla</a> or the <a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws paper</a>. There are also a bunch of good theory-based papers, though the one I am familiar with is actually this Jane Street <a href="https://blog.janestreet.com/does-batch-size-matter/">blog post on understanding batch size</a>.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong>Pre-training with Human Feedback</strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>Pre-training tends to have a very unsupervised format, though <a href="https://arxiv.org/pdf/2302.08582.pdf">PHF</a>(Feb 2023) applies a simple technique to label data at pretraining. It uses two conditioning tokens (good and bad) prepended to samples at training and then samples with them at inference. They tried various other objectives (notably, filtering out bad data) that all performed worse, evaluated on python styling, PII and toxicity.</p>
<p><strong><strong><strong>MuP</strong></strong></strong></p>
<p><a href="https://arxiv.org/pdf/2203.03466.pdf">Maximal Update Parameterization</a> is a method of parameterization that makes hyperparameters (the ones related to learning rates and optimisers) predictable (consistent) across model sizes. It not only saves the parameter sweep compute but should also be closer to optimal. The paper does a really good job getting into the theory of why this works.</p>
<h2 id="none-of-the-above">None of the Above</h2>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Chain of Thought</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>This is a technique where it makes the model think “step-by-step” and yielding better results. That name originated in <a href="https://arxiv.org/pdf/2201.11903.pdf">this paper</a>, which describes a specific application of the technique described in this February 2021 <a href="https://arxiv.org/pdf/2102.07350.pdf">paper</a> which describes ways to do prompting that aren’t just few-shotting. The phrase now is sometimes used to describe techniques that aren’t just prompting.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong>Tool Use</strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>A good canonical tool use paper is probably the Dec 2021 <a href="https://arxiv.org/pdf/2112.09332.pdf">WebGPT paper</a> (though the earliest paper I can find is probably this <a href="https://proceedings.mlr.press/v70/shi17a.html">2017 Karpathy paper</a>), in which capabilities are greatly enhanced by giving GPT-3 access to the web. It is finetuned with some RL and SL, though I put this not as a training or post-pretraining technique since the concept is not dependent on that. DeepMind also trained <a href="https://arxiv.org/pdf/2202.08137.pdf">RL tool use agents</a>, and Meta has <a href="https://arxiv.org/pdf/2302.04761.pdf">toolformer</a> which does finetuning focused on API usage.</p>
<p><strong>Fill In the Middle</strong></p>
<p>This July 2022 <a href="https://arxiv.org/pdf/2207.14255.pdf">paper</a> describes a simple data transformation which moves a substring from the middle of a text to the end, and asks the model to fill in the middle. This allows the model to gain a capability that is really useful for tasks like code completion without damage to performance on strictly left to right tasks.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong>Sampling Techniques: Top-k, Top-p (nucleus), Beam Search</strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p>The output of language models is fundamentally logits for every possible token, which are then softmaxed into becoming probabilities. The most naive way of turning your logits into tokens, is to take the most likely token. When there are temperature controls with language models, it’s dividing the logits by the temperature, which makes the model more/less confident in its top choice. Top-K sampling takes the top K tokens and samples from that distribution. Top-P sampling (it has a <a href="https://arxiv.org/pdf/1904.09751.pdf">paper</a> but it’s probably useless), or nucleus sampling, uses the top P percentage (think CDFs) of tokens and samples from there.</p>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>Tail Free Sampling</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p>
<p><a href="https://www.trentonbricken.com/Tail-Free-Sampling/">Tail Free Sampling</a> takes the derivative of top-P sampling, and is named as such to find the “tail”, as top-P sampling could fail in cases of being cut off at a point where many tokens have similar probabilities. The post linked details the theoretical reasons this should result in better sampling, but when it comes to improving creativity and range in the models there are no good benchmarks.</p>
<hr />
<p>This feels particularly good to publish now since i feel like its ~the last time something like this could be helpful, given how close we are and the diminishing amount of published research.</p>
<p><em>Edited by Claude &lt;3</em></p>


    </section>
  </div>
</article>


<footer>
  <hr>
    <h1 class="title-link"><a href="https:&#x2F;&#x2F;kipp.ly">kipply&#x27;s blog</a></h1>
    <div class="header-links">
      <a href="https://twitter.com/kipperrii">twitter</a>,
      <a href="mailto:email@kipp.ly">email </a>
    </div>
</footer>
<!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "1afaa6bb0295489fa7a15cd4b46fe09c"}'></script><!-- End Cloudflare Web Analytics -->
  </main>
<!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "1afaa6bb0295489fa7a15cd4b46fe09c"}'></script><!-- End Cloudflare Web Analytics -->
</body>
</html>


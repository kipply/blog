+++
title = "Things Read | Jul/Aug 2022"
date = 2022-08-03
weight = 2
+++

[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://www.anthropic.com/red_teaming.pdf), in which Anthropic finds that larger models are harder to redteam and releases a redteaming dataset. A little more clarity about [why and how to worry about alignment](https://drive.google.com/file/d/1TsB7WmTG2UzBtOs349lBqY5dEBaxZTzG/view) from a technical perspective from Richard. Anthropic’s [SoLU Interpretability Paper](https://transformer-circuits.pub/2022/solu/index.html), with more intense math than last time but I’m managing. [Set Sail for Fail](https://nintil.com/ai-safety), a really long nintil post about worrying about AI risk, with some hands tied behind his back. Jack Clark’s [tweet thread](https://twitter.com/jackclarkSF/status/1555980412333133824) on AI Policy. [A Mechanistic Interpretability Analysis of Grokking](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking) from Neel Nanda, who I made explain fourier bases to me.

[LLM.int8() and](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/) [Emergent Features](https://arxiv.org/pdf/2208.07339.pdf), basically we should open the box (weights) and then think about solutions. [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/pdf/2207.14255.pdf), a teaser of how we can get more out of our data. [Language Model Cascades](https://arxiv.org/pdf/2207.10342.pdf), a framework for thinking about and building inference(ish) time model-thought-paths. [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/pdf/2201.02177.pdf), in which models generalize after overfitting (phase changes?). [Language models show human-like content effects on reasoning](https://arxiv.org/pdf/2207.07051.pdf%60), really nice DeepMind paper that captures how language models are flawed in the same way humans are, referencing 70’s psych papers. [RHO-LOSS](https://arxiv.org/pdf/2206.07137.pdf) for better data selection. Salesforce [Code RL paper](https://arxiv.org/pdf/2207.01780.pdf).

José Luis Ricón Fernández de la Puente’s [O1 Visas](https://nintil.com/us-immigration/) post at last. Someone [explained Jane Street](https://www.thediff.co/p/jane-street?triedSigningIn=true) in a way that didn’t make me sad. wtf someone [dislikes food](https://jayriverlong.substack.com/p/42-against-food). Aella on [Learning the Elite Class](https://aella.substack.com/p/learning-the-elite-class). [Scratch is a Big Deal](https://www.bryanbraun.com/2022/07/16/scratch-is-a-big-deal/). Someone captured a lot of the important aspects of [Miyazaki films](https://www.robinsloan.com/newsletters/visions/#miyazaki), time for me to start rewatching.

A [Vox article about EA](https://www.vox.com/future-perfect/2022/8/8/23150496/effective-altruism-sam-bankman-fried-dustin-moskovitz-billionaire-philanthropy-crytocurrency), don’t think I’ve ever related so much to a journalist. The [Times profile on MacAskill](https://time.com/6204627/effective-altruism-longtermism-william-macaskill-interview/) was such a throwback. Kelsey Piper on the [divides in AI Safety](https://www.vox.com/future-perfect/2022/8/10/23298108/ai-dangers-ethics-alignment-present-future-risk), usually epistemic and moral philosophy is a stronger divider than cause area. The [Elon <> EA story](https://archive.ph/pY4gF) finally came out and it’s sad but dramatic? The [CHIPS Science Act](https://hai.stanford.edu/sites/default/files/2022-08/HAI%20Explainer%20-%20What%20The%20CHIPS%20and%20Science%20Act%20Means%20for%20AI.pdf), a plan to spend 280B on semiconductor manufacturing.
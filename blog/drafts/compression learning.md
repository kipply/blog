+++
title = "Compression Algorithms and a Poem About AI"
date = 2022-07-09
weight = 5
+++

I think the most confusing thing about EA is _why we do this_. It's definitely not clear internally, or also externally and I think the [harshest](https://twitter.com/timnitGebru/status/1518832724206792704) [critiques](https://reboothq.substack.com/p/ineffective-altruism?s=r) of EA stem from this problem.

Though a little dangerous, I will address the critiques directly. In the case of Timnit, I think she doesn't actually believe that EAs are here to do the most good. I don't agree with her other conclusions, but I think she is quite correct to question whether EAs are motivated by "doing the most good", which can be separated from whether EAs are attempting to do the most good anyway. The Reboot article I think is similarly confused. Though I'm reading between the lines a lot, I think their fondness of "Ineffective Altruism" comes from the fact that subscribing to Ineffective Altruism has clearer motivations. The motivations there shift towards doing good things that feel good, to making immediate impact, motivations that everyone can understand, whereas "I'm giving it all up to maximize the good I do in the world, whatever that looks like" is almost... suspicious?

Internally, I think there are reasonable discussions about understanding the motivations of EA. Perhaps there is some lore to this, but I've had some large number of conversations with people trying to consider how much attention to AI Alignment stems from [nerd sniping](https://xkcd.com/356/) as a motivation rather than Doing The Most Good We Can. It would take a lot of powerful introspection to come up with an answer for an individual, and I'm not even sure what it would take to understand the motivations of a group thinking about these problems.

In tangent to alignment as a cause area, many EAs question their motivations on their own accord. There's some amount of tension between people who don't work on existential risk and people who do. When it came to comparing global poverty to animal welfare, it was clear that both of them mattered and should be worked on and quibbles about which was more effective and tractable were just quibbles. When existential risk arose as a cause area, it by nature made a bit of a claim of "this might be the only thing that matters". With that I think many were forced to introspect and question their motivations.

Of the EAs who believed in existential risk as an important cause area, were they going to find that their motivation was doing the most good they could and should therefore pivot to x-risk? Some of them did surely, as we've noticed grant-makers and philosophers shift focus towards x-risk and longtermism causes. We have a [census](https://forum.effectivealtruism.org/posts/83tEL2sHDTiWR6nwo/ea-survey-2020-cause-prioritization) that shows us that many EAs still consider things like poverty, climate change and animal welfare to be top causes, but I'd be curious about the number of people who consider x-risk to be a top cause and work in a different cause area.

I've met a few of these people, and I think their top motivator is a genuine passion for what they work on. They have history with the cause area, emotional attachments, and a career built around it. I said "I'm giving it all up to maximize the good I do in the world, whatever that looks like" is a little suspicious, but I think "I'm passionate about global poverty because my parents were immigrants from a third world country" is thoroughly unsuspicious. I also consider it genuinely no less moral than the canonical EA-good-maximizer motivation.

Going back to AI alignment (which is a cause area I'm about to start a job in) I find that it concerns people when I say "I think AI is the important problem of our generation", rather than "I happen to work in AI, I'm quite passionate about it and this is the obviously pressing problem here" even though I wholly believe both phrases. The second is actually quite tangential to "I believe in alignment because I was nerd sniped into it", but what nerd sniped means here isn't using morality to give myself an excuse, but using my interests as a motivator to participate in EA. I still really care about doing good! It's at least the case that I care about it more than making money, but I also don't believe that "doing the most good" is my prime motivator.

Passion about a cause area is a really good reason to be an EA. I think if EAs acknowledged that, and also considered it a stance that is no morally worse than solely maximising for good that it would make EA more palatable and a more comfortable place for its community.

The conversation that prompted this post, was when I was talking to someone who was understanding EA for the first time. This person was religious and noted that they were impressed that there are people who do this without the motivations of faith in their life (though there are a good [number of religious EAs](https://rethinkpriorities.org/publications/eas2019-community-demographics-characteristics#:~:text=A%20majority%20are%20between%20the,%2Fatheist%2Fnon%2Dreligious.)). This is to say, that I think another motivation of EA is the community and when people relate it to religion (or cult to be derogatory) they're actually noticing something quite real.

A central belief I have is that it is extremely hard to find reasons to be good. It often happens through social dynamics, through religion, through interest and it's really hard to pin down how it might happen with more sincerity -- if that's even a concept you believe in. Because of this, I think EA should acknowledge the motivations, whether it be out of community, intellectual interest while keeping in mind that we do share a goal of doing the most good we can. Understanding these motivations can help us make better decisions in how we interact with EA or even in broader cause prioritisation but also will immediately make EA a more comfortable place to be. Owning our "reason to be good" makes EA a bit more human, and clears up some senses of self-righteousness (see [Michael Nielsen's footnote #13](https://michaelnotebook.com/eanotes/))  and will help us maintain focus on our shared goal.

The reason I'm worried about the focus on our shared goal is that EA suddenly flush with cash, and we've added some new items to our reasons to be good: money and status (status is only somewhat new). When I think of this, I think of [Ben Hilton](https://80000hours.org/author/benjamin-hilton/) saying that his least favourite thing about 80k was that they pay people too much and the offices are too nice. It's become more possible to be very involved in EA and have a lot of cash, whether personally or for your project. It also means that EA has become vastly more high status, though the general attention to EA has also contributed to it. An intellectual, tight-knit community that focuses on morality was a status-y to start, but the widerspread awareness of EA makes a big splash.

It feels hypocritical to say that passion and community are good reasons to be an EA, when money and status are not. I think the situation here, is that all four of those things are equally virtuous of a reason to be an EA for the individual, and it's very possible that it's very beneficial that there are more+better reasons now. It is also the case though, that community and passion mechanistically keep us focused on the goal of doing the most good. Investment in the community requires investment in its ideologies and passion about an EA cause area requires it to have been a cause area in the first place. Maybe money and status are similarly tied? You get money as an EA by working on the cause areas that need the most funding, and as long as the EA status ladder is aligned with what we believe is doing the most good, everything should be fine. Of course, priors imply that money and status are more likely to go wrong than community and passion.

It would be hard, but it would be nice to gather some survey data on why people are EAs so that maybe it can be less confusing. This is definitely a weird territory of thinking because it asks questions about morality on more levels of meta than I prefer to work on, and I wonder if there's good thinking to be done here at a higher surface level. I'd like to reiterate that it's really hard to find reasons to be good, and that expectedly bothers people who are externally observing EA. Additional note that I think this internally bothers EAs too as I have at many points felt concerned about my morality due to my motivations.

